model: DeepSeek-R1-昇腾版
label:
  zh_Hans: DeepSeek-R1-昇腾版
  en_US: DeepSeek-R1-昇腾版
model_type: llm
features:
  - agent-thought
model_properties:
  mode: chat
  context_size: 8388000
parameter_rules:
  - name: temperature
    use_template: temperature
    type: float
    default: 1
    min: 0.0
    max: 2.0
    help:
      zh_Hans: 控制生成结果的多样性和随机性。数值越小，越严谨；数值越大，越发散。
      en_US: Control the diversity and randomness of generated results. The smaller the value, the more rigorous it is; the larger the value, the more divergent it is.
  - name: max_tokens
    use_template: max_tokens
    type: int
    default: 4096
    min: 1
    max: 16384
    help:
      zh_Hans: 指定生成结果长度的上限。如果生成结果截断，可以调大该参数。
      en_US: Specifies the upper limit on the length of generated results. If the generated results are truncated, you can increase this parameter.
  - name: top_p
    use_template: top_p
    type: float
    default: 1
    min: 0.01
    max: 1.00
    help:
      zh_Hans: 控制生成结果的随机性。数值越小，随机性越弱；数值越大，随机性越强。一般而言，top_p 和 temperature 两个参数选择一个进行调整即可。
      en_US: Control the randomness of generated results. The smaller the value, the weaker the randomness; the larger the value, the stronger the randomness. Generally speaking, you can adjust one of the two parameters top_p and temperature.
  - name: top_k
    use_template: top_k
    type: int
    default: 20
    min: 1
    max: 100
    help:
      zh_Hans: 	超参：top_k 采样。预置大模型取值范围 [1, 100]。默认值为20。取值越大，生成的随机性越高；取值越小，生成的确定性越高。
      en_US: Control the randomness of generated results. The smaller the value, the weaker the randomness; the larger the value, the stronger the randomness. Generally speaking, you can adjust one of the two parameters top_p and temperature.
  - name: presence_penalty
    use_template: presence_penalty
    default: 0
    min: -2.0
    max: 2.0
    help:
      zh_Hans: 频率惩罚：取值范围[-2.0, 2.0]，默认值为`0.0`,它影响模型如何根据文本中词汇（token）的现有频率惩罚新词汇（token）。值大于`0`，会根据新标记在文本中的现有频率来惩罚新标记，从而降低模型逐字重复同一行的可能性。。
      en_US: Take the value range [-2.0, 2.0], with the default value of `0.0`, which affects how the model punishes new vocabulary (tokens) based on the existing frequency of the vocabulary (tokens) in the text. A value greater than `0` will punish new tags based on the existing frequency of the new tag in the text, reducing the possibility that the model will repeat the same line verbatim.
  - name: frequency_penalty
    use_template: frequency_penalty
    default: 0
    min: -2.0
    max: 2.0
    help:
      zh_Hans: 介于 -2.0 和 2.0 之间的数字。如果该值为正，那么新 token 会根据其在已有文本中的出现频率受到相应的惩罚，降低模型重复相同内容的可能性。
      en_US: A number between -2.0 and 2.0. If the value is positive, new tokens are penalized based on their frequency of occurrence in existing text, reducing the likelihood that the model will repeat the same content.
  - name: response_format
    use_template: response_format
pricing:
  input: "4"
  output: "16"
  unit: "0.000001"
  currency: RMB
